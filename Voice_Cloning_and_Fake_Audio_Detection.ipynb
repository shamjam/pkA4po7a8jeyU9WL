{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "the8fOIUdUAk"
      },
      "source": [
        "# Project 6: VCFAD - Voice Cloning and Fake Audio Detection\n",
        "(TODO)\n",
        "\n",
        "### Data Description\n",
        "There are two datasets we will use for this project:\n",
        "\n",
        "**TIMIT Dataset**\n",
        "\n",
        "This dataset contains 6300 sentences (10 sentences spoken by 630 different speakers) and will be used to create a voice cloner that generates fake samples.\n",
        "\n",
        "**CommonVoice Dataset**\n",
        "\n",
        "This dataset contains ... is used for our real voice samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uztm79-OeB3d"
      },
      "source": [
        "### Project Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OKrYm-J9K2ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc46fde7-4822-4c9c-f42c-229bde46761e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/real’: File exists\n",
            "mkdir: cannot create directory ‘data/fake’: File exists\n",
            "mkdir: cannot create directory ‘data/fake_by_speaker’: File exists\n",
            "mkdir: cannot create directory ‘data/fake_by_speaker/0’: File exists\n",
            "mkdir: cannot create directory ‘data/fake_by_speaker/1’: File exists\n",
            "mkdir: cannot create directory ‘data/fake_by_speaker/2’: File exists\n",
            "mkdir: cannot create directory ‘data/fake_by_speaker/3’: File exists\n",
            "mkdir: cannot create directory ‘data/fake_by_speaker/4’: File exists\n",
            "mkdir: cannot create directory ‘data/fake_by_speaker/5’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir \"data\"\n",
        "!mkdir \"data/real\"\n",
        "!mkdir \"data/fake\"\n",
        "!mkdir \"data/fake_by_speaker\"\n",
        "!mkdir \"data/fake_by_speaker/0\"\n",
        "!mkdir \"data/fake_by_speaker/1\"\n",
        "!mkdir \"data/fake_by_speaker/2\"\n",
        "!mkdir \"data/fake_by_speaker/3\"\n",
        "!mkdir \"data/fake_by_speaker/4\"\n",
        "!mkdir \"data/fake_by_speaker/5\"\n",
        "#!mkdir \"data/me_real\"\n",
        "#!mkdir \"data/me_fake\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI32tChEnv0j",
        "outputId": "359d3cf1-ec31-46e0-869e-7f509a3ca3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: TTS in /usr/local/lib/python3.10/dist-packages (0.13.3)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.0.0+cu118)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.10.1)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from TTS) (0.3.2)\n",
            "Requirement already satisfied: cython==0.29.28 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.29.28)\n",
            "Requirement already satisfied: bangla==0.0.2 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.0.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS) (2.0.1+cu118)\n",
            "Requirement already satisfied: inflect==5.6.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (5.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from TTS) (4.65.0)\n",
            "Requirement already satisfied: gruut[de]==2.2.3 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.2.3)\n",
            "Requirement already satisfied: jamo in /usr/local/lib/python3.10/dist-packages (from TTS) (0.4.1)\n",
            "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.56.4)\n",
            "Requirement already satisfied: coqpit>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.0.17)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from TTS) (3.7.1)\n",
            "Requirement already satisfied: unidic-lite==1.0.8 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.0.8)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.10/dist-packages (from TTS) (0.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from TTS) (1.21.0)\n",
            "Requirement already satisfied: umap-learn==0.5.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.5.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from TTS) (2.2.4)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.10/dist-packages (from TTS) (0.48.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from TTS) (1.5.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from TTS) (6.0)\n",
            "Requirement already satisfied: mecab-python3==1.0.5 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.0.5)\n",
            "Requirement already satisfied: bnunicodenormalizer==0.1.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.1.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from TTS) (0.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from TTS) (23.1)\n",
            "Requirement already satisfied: librosa==0.10.0.* in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.0.post2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.1)\n",
            "Requirement already satisfied: bnnumerizer in /usr/local/lib/python3.10/dist-packages (from TTS) (0.0.2)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2023.4.0)\n",
            "Requirement already satisfied: g2pkk>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.1.2)\n",
            "Requirement already satisfied: trainer==0.0.20 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.0.20)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n",
            "Requirement already satisfied: gruut-lang-en~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (2.0.0)\n",
            "Requirement already satisfied: jsonlines~=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (1.2.0)\n",
            "Requirement already satisfied: dateparser~=1.1.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (1.1.8)\n",
            "Requirement already satisfied: networkx<3.0.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (2.8.8)\n",
            "Requirement already satisfied: num2words<1.0.0,>=0.5.10 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (0.5.12)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (2.12.1)\n",
            "Requirement already satisfied: python-crfsuite~=0.9.7 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (0.9.9)\n",
            "Requirement already satisfied: gruut-ipa<1.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (0.13.0)\n",
            "Requirement already satisfied: gruut-lang-de~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (2.0.0)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.6.0)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.0.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (4.4.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (0.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.2.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (4.5.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4->TTS) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4->TTS) (67.7.2)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (from trainer==0.0.20->TTS) (2.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer==0.0.20->TTS) (5.9.5)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from trainer==0.0.20->TTS) (3.19.6)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.1->TTS) (0.5.10)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->TTS) (1.15.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (3.12.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->TTS) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->TTS) (16.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (1.9.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (23.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (1.3.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (8.1.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (2.3.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (2.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->TTS) (2022.10.31)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->TTS) (2022.7.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->TTS) (2.21)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (4.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->TTS) (2.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut[de]==2.2.3->TTS) (1.16.0)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words<1.0.0,>=0.5.10->gruut[de]==2.2.3->TTS) (0.6.2)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa==0.10.0.*->TTS) (3.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->TTS) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->TTS) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (1.26.15)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (2023.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy==1.21.0 in /usr/local/lib/python3.10/dist-packages (1.21.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.26.15)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.19.6)\n",
            "Collecting protobuf\n",
            "  Using cached protobuf-4.22.3-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trainer 0.0.20 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.3 which is incompatible.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.3 which is incompatible.\n",
            "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.22.3 which is incompatible.\n",
            "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.22.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (23.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Using cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.32.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (16.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.21.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (67.7.2)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.8.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (23.3.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (1.54.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (4.5.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.40.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.27.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.22.3\n",
            "    Uninstalling protobuf-4.22.3:\n",
            "      Successfully uninstalled protobuf-4.22.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.19.6\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary packages\n",
        "#!pip install gtts\n",
        "!pip install pydub\n",
        "!pip install TTS\n",
        "!pip3 install numpy==1.21.0\n",
        "!pip install SpeechRecognition\n",
        "!pip install --upgrade protobuf\n",
        "!pip install tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lRFQVmEgal8a"
      },
      "outputs": [],
      "source": [
        "# Import the necessary packages\n",
        "import pandas as pd\n",
        "from IPython.display import Audio\n",
        "import subprocess\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import seed\n",
        "from random import randint\n",
        "from TTS.api import TTS\n",
        "from IPython import display\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import speech_recognition as sr\n",
        "from os import path\n",
        "from pydub import AudioSegment\n",
        "from imutils import paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-Qq7vAmyeGDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0da58cb-e190-4c59-a621-5dd34777ba69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and import datasets\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zghC3PH0taiY"
      },
      "outputs": [],
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/gdrive/MyDrive/Datasets/archive.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/timit\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "cwU97Mjur3c-"
      },
      "outputs": [],
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/gdrive/MyDrive/Datasets/cv-corpus-12.0-delta-2022-12-07.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/cv\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wq_L876qJVdQ"
      },
      "outputs": [],
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/gdrive/MyDrive/Datasets/cv-corpus-10.0-delta-2022-07-04.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/cv\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jvNMZ1GrJ8gg"
      },
      "outputs": [],
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/gdrive/MyDrive/Datasets/cv-corpus-11.0-delta-2022-09-21.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/cv\")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uZnqafmUuH_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "9e1a8888-11e9-4c0f-d2fd-c7731cdd08cd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-8ed335bfb046>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# For each file in TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ffmpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{path}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-acodec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pcm_s16le'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-ar'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'16000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-ac'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'data/real/{i}.wav'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ffmpeg', '-i', '/content/cv/cv-corpus-10.0-delta-2022-07-04/en/clips/common_voice_en_32762635.mp3', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', 'data/real/0.wav']' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "cv_paths = list(paths.list_files(\"/content/cv/\", \".mp3\"))\n",
        "i=0\n",
        "# For each file in TRAIN\n",
        "for path in cv_paths:\n",
        "  subprocess.check_call(['ffmpeg', '-i', f'{path}', '-acodec', 'pcm_s16le', '-ar' ,'16000', '-ac', '1', f'data/real/{i}.wav'])\n",
        "  i += 1\n",
        "  if i > 1500:\n",
        "    break;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plo_JoxptMqk"
      },
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WxHYu4ofTDD"
      },
      "source": [
        "For the Common Voice dataset, we'll use the Delta Segment 12.0. And from that, we'll use only the validated clips."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbC_UTPhfgTU",
        "outputId": "5de738a9-6758-4019-f81a-9cb204428dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 6717\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/cv/cv-corpus-12.0-delta-2022-12-07/en/validated.tsv', sep='\\t', header=0)\n",
        "\n",
        "print(f\"Total samples: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztBNTystvTE2"
      },
      "source": [
        "There are 6717 samples in this dataset. Now lets check if these are unique sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfOnDkjn63sT",
        "outputId": "c6672a46-bfd2-4c76-d7e4-2bd0c2f3f06a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Training opportunities occur every February and March.                                     3\n",
              "The house itself was expanded twice during the nineteenth century.                         2\n",
              "She was the Associations's first Secretary.                                                2\n",
              "Especially tall fly towers pose a balance problem for standard counterweight line sets.    2\n",
              "In this film she portrayed a magazine illustrator seeking western types.                   2\n",
              "                                                                                          ..\n",
              "This was enough to win the Olympic bronze medal.                                           1\n",
              "He was survived by his four children Frances, Helen, Joseph and Veronica.                  1\n",
              "We hope this campaign will help us achieve that.                                           1\n",
              "I have it still.                                                                           1\n",
              "The station was originally named Palmers Green and Southgate.                              1\n",
              "Name: sentence, Length: 6675, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "df.sentence.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D_CCtC87Iea"
      },
      "source": [
        "There are multiple with non-unique sentences. Lets reduce our DataFrame so only unique sentences remain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1ELq0qT7INL",
        "outputId": "d2b55d79-373d-43f7-d921-55eca81b1401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 6675\n"
          ]
        }
      ],
      "source": [
        "df = df.drop_duplicates(\"sentence\")\n",
        "print(f\"Total samples: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbE-S8k-67Kx"
      },
      "source": [
        "There are now 6675 samples remaining. Lets take the first 1000 with unique sentences and convert each clip to .wav format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "X1yy6psivScp"
      },
      "outputs": [],
      "source": [
        "df = df.head(1000)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "f23maYMkKb77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "90d9c62c-791f-40bc-c671-bafb60c34e9a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a76a58ffff0c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert clips to wav format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ffmpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'/content/cv/cv-corpus-12.0-delta-2022-12-07/en/clips/{path}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-acodec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pcm_s16le'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-ar'\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'16000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-ac'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'data/real/{path}.wav'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ffmpeg', '-i', '/content/cv/cv-corpus-12.0-delta-2022-12-07/en/clips/common_voice_en_35244283.mp3', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', 'data/real/common_voice_en_35244283.mp3.wav']' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "# Convert clips to wav format\n",
        "for path in df.path:\n",
        "  subprocess.check_call(['ffmpeg', '-i', f'/content/cv/cv-corpus-12.0-delta-2022-12-07/en/clips/{path}', '-acodec', 'pcm_s16le', '-ar' ,'16000', '-ac', '1', f'data/real/{path}.wav'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3SpgzZERlfb"
      },
      "outputs": [],
      "source": [
        "# Calculate duration for each clip\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# specify the folder path containing audio files\n",
        "folder_path = '/content/data/real'\n",
        "\n",
        "# get all audio files in the folder\n",
        "audio_files = [f for f in os.listdir(folder_path) if f.endswith(('.wav'))]\n",
        "\n",
        "# iterate over the audio files and get their duration\n",
        "for audio_file in audio_files:\n",
        "    audio = AudioSegment.from_file(os.path.join(folder_path, audio_file))\n",
        "    duration = len(audio) / 1000.0\n",
        "    df.loc[df['path'] == audio_file[:-4], 'duration'] = duration\n",
        "    #print(f\"{audio_file[:-4]}: {duration:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSo1x0zmTW2q"
      },
      "outputs": [],
      "source": [
        "# Plot a histogram for the duration of the real audio files\n",
        "import plotly.express as px\n",
        "#df = px.data.tips()\n",
        "fig = px.histogram(df, x=\"duration\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-zGvj7Sh9B8"
      },
      "source": [
        "### Build a voice cloning system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16dZCOAd9QJ"
      },
      "source": [
        "#### Strategy 1: Use text to speech library to generate 'fake' voice clips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5mCDBRx0vl",
        "outputId": "95c1aa57-4624-4212-c56e-7be76a00a90f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No API token found for 🐸Coqui Studio voices - https://coqui.ai \n",
            "Visit 🔗https://app.coqui.ai/account to get one.\n",
            "Set it as an environment variable `export COQUI_STUDIO_TOKEN=<token>`\n",
            "\n",
            " > tts_models/multilingual/multi-dataset/your_tts is already downloaded.\n",
            " > Using model: vits\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:0\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:None\n",
            " | > fft_size:1024\n",
            " | > power:None\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:None\n",
            " | > signal_norm:None\n",
            " | > symmetric_norm:None\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:None\n",
            " | > pitch_fmin:None\n",
            " | > pitch_fmax:None\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Model fully restored. \n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:64\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:512\n",
            " | > power:1.5\n",
            " | > preemphasis:0.97\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:False\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:False\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:True\n",
            " | > db_level:-27.0\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:160\n",
            " | > win_length:400\n",
            " > External Speaker Encoder Loaded !!\n",
            " > initialization of language-embedding layers.\n",
            " > Model fully restored. \n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:16000\n",
            " | > resample:False\n",
            " | > num_mels:64\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:20\n",
            " | > fft_size:512\n",
            " | > power:1.5\n",
            " | > preemphasis:0.97\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:False\n",
            " | > mel_fmin:0\n",
            " | > mel_fmax:8000.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:20.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:False\n",
            " | > do_trim_silence:False\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:True\n",
            " | > db_level:-27.0\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:160\n",
            " | > win_length:400\n"
          ]
        }
      ],
      "source": [
        "# List available 🐸TTS models and choose the first one\n",
        "model_name = TTS.list_models()[0]\n",
        "\n",
        "# Init TTS\n",
        "tts = TTS(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "F9Ru45dmx42x",
        "outputId": "b730c526-2be9-4657-a340-05f4181855f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > Text splitted to sentences.\n",
            "['He was commonly referred to as the \"Blacksmith of Ballinalee\".']\n",
            " > Processing time: 1.9502630233764648\n",
            " > Real-time factor: 0.49942715067259025\n",
            " > Text splitted to sentences.\n",
            "['He was commonly referred to as the \"Blacksmith of Ballinalee\".']\n",
            " > Processing time: 1.339958906173706\n",
            " > Real-time factor: 0.3335720453506861\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d2c101442ec7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mspeaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"data/fake/{i}.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeakers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"data/fake/{speaker}/{i}.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Save fake path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/api.py\u001b[0m in \u001b[0;36mtts_to_file\u001b[0;34m(self, text, speaker, language, speaker_wav, emotion, speed, file_path)\u001b[0m\n\u001b[1;32m    577\u001b[0m             )\n\u001b[1;32m    578\u001b[0m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_wav\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeaker_wav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/utils/synthesizer.py\u001b[0m in \u001b[0;36msave_wav\u001b[0;34m(self, wav, path)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[1;32m    206\u001b[0m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0msave_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_sample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvoice_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_wav\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_wav\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/TTS/utils/audio/numpy_transforms.py\u001b[0m in \u001b[0;36msave_wav\u001b[0;34m(wav, path, sample_rate, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \"\"\"\n\u001b[1;32m    438\u001b[0m     \u001b[0mwav_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32767\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m     \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwav_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(filename, rate, data)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/fake/1/0.wav'"
          ]
        }
      ],
      "source": [
        "seed(1)\n",
        "i=0\n",
        "for sentence in df.sentence:\n",
        "  speaker = randint(0, 5)\n",
        "  tts.tts_to_file(text=sentence, speaker=tts.speakers[speaker], language=tts.languages[0], file_path=f\"data/fake/{i}.wav\")\n",
        "  tts.tts_to_file(text=sentence, speaker=tts.speakers[speaker], language=tts.languages[0], file_path=f\"data/fake/{speaker}/{i}.wav\")\n",
        "  \n",
        "  # Save fake path\n",
        "  df.loc[i,'fake_path'] = f\"data/fake/{i}.wav\"\n",
        "  #df.loc[i,'fake_path'] = f\"data/fake/{speaker}/{i}.wav\"\n",
        "\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWVPeS70nli3"
      },
      "outputs": [],
      "source": [
        "Audio('/content/data/fake/844.wav', autoplay=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V2xlxpY4HUz"
      },
      "outputs": [],
      "source": [
        "Audio('/content/data/fake/845.wav', autoplay=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvVz9dn34KCS"
      },
      "outputs": [],
      "source": [
        "Audio('/content/data/fake/846.wav', autoplay=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLkc9iXp9zU_"
      },
      "outputs": [],
      "source": [
        "# Calculate duration for each clip\n",
        "\n",
        "# specify the folder path containing audio files\n",
        "folder_path = '/content/data/fake'\n",
        "\n",
        "# get all audio files in the folder\n",
        "audio_files = [f for f in os.listdir(folder_path) if f.endswith(('.wav'))]\n",
        "\n",
        "# iterate over the audio files and get their duration\n",
        "durations = []\n",
        "for audio_file in audio_files:\n",
        "    audio = AudioSegment.from_file(os.path.join(folder_path, audio_file))\n",
        "    duration = len(audio) / 1000.0\n",
        "    #df.loc[df['path'] == audio_file[:-4], 'duration'] = duration\n",
        "    #print(f\"{audio_file[:-4]}: {duration:.2f} seconds\")\n",
        "    durations.append(duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scaSC3Dz9-Lj"
      },
      "outputs": [],
      "source": [
        "# Plot a histogram for the duration of the fake audio files\n",
        "import plotly.express as px\n",
        "#df = px.data.tips()\n",
        "fig = px.histogram(durations).update_layout(\n",
        "    xaxis_title=\"Duration\"\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2lPHSHBeMNM"
      },
      "source": [
        "##### Calculate word error rate\n",
        "- Audio transcription\n",
        "- Match with original text (each word)\n",
        "- Calculate error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idifrf4rLenL"
      },
      "outputs": [],
      "source": [
        "def wer(ref, hyp ,debug=True):\n",
        "    r = ref.split()\n",
        "    h = hyp.split()\n",
        "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
        "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
        "    # backtrace will hold the operations we've done.\n",
        "    # so we could later backtrace, like the WER algorithm requires us to.\n",
        "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
        " \n",
        "    OP_OK = 0\n",
        "    OP_SUB = 1\n",
        "    OP_INS = 2\n",
        "    OP_DEL = 3\n",
        "    DEL_PENALTY = 1\n",
        "    INS_PENALTY = 1\n",
        "    SUB_PENALTY = 1\n",
        "    \n",
        "    # First column represents the case where we achieve zero\n",
        "    # hypothesis words by deleting all reference words.\n",
        "    for i in range(1, len(r)+1):\n",
        "        costs[i][0] = DEL_PENALTY*i\n",
        "        backtrace[i][0] = OP_DEL\n",
        "    \n",
        "    # First row represents the case where we achieve the hypothesis\n",
        "    # by inserting all hypothesis words into a zero-length reference.\n",
        "    for j in range(1, len(h) + 1):\n",
        "        costs[0][j] = INS_PENALTY * j\n",
        "        backtrace[0][j] = OP_INS\n",
        "    \n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                costs[i][j] = costs[i-1][j-1]\n",
        "                backtrace[i][j] = OP_OK\n",
        "            else:\n",
        "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
        "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
        "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
        "                 \n",
        "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
        "                if costs[i][j] == substitutionCost:\n",
        "                    backtrace[i][j] = OP_SUB\n",
        "                elif costs[i][j] == insertionCost:\n",
        "                    backtrace[i][j] = OP_INS\n",
        "                else:\n",
        "                    backtrace[i][j] = OP_DEL\n",
        "                 \n",
        "    # back trace though the best route:\n",
        "    i = len(r)\n",
        "    j = len(h)\n",
        "    numSub = 0\n",
        "    numDel = 0\n",
        "    numIns = 0\n",
        "    numCor = 0\n",
        "    if debug:\n",
        "        print(\"OP\\tREF\\tHYP\")\n",
        "        lines = []\n",
        "    while i > 0 or j > 0:\n",
        "        if backtrace[i][j] == OP_OK:\n",
        "            numCor += 1\n",
        "            i-=1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
        "        elif backtrace[i][j] == OP_SUB:\n",
        "            numSub +=1\n",
        "            i-=1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
        "        elif backtrace[i][j] == OP_INS:\n",
        "            numIns += 1\n",
        "            j-=1\n",
        "            if debug:\n",
        "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
        "        elif backtrace[i][j] == OP_DEL:\n",
        "            numDel += 1\n",
        "            i-=1\n",
        "            if debug:\n",
        "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
        "    if debug:\n",
        "        lines = reversed(lines)\n",
        "        for line in lines:\n",
        "            print(line)\n",
        "        print(\"#cor \" + str(numCor))\n",
        "        print(\"#sub \" + str(numSub))\n",
        "        print(\"#del \" + str(numDel))\n",
        "        print(\"#ins \" + str(numIns))\n",
        "    # return (numSub + numDel + numIns) / (float) (len(r))\n",
        "    wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
        "    return {'WER':wer_result, 'numCor':numCor, 'numSub':numSub, 'numIns':numIns, 'numDel':numDel, \"numCount\": len(r)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVxoy5KtNp5L"
      },
      "outputs": [],
      "source": [
        "# Calculate word error rate for all sentences\n",
        "i=0\n",
        "r = sr.Recognizer()\n",
        "for sentence in df.sentence:  \n",
        "  # Transcribe fake audio                                       \n",
        "  with sr.AudioFile(f\"data/fake/{i}.wav\") as source:\n",
        "    audio = r.record(source)  # read the entire audio file\n",
        "    try:\n",
        "      df.loc[i,'fake_transcription'] = r.recognize_google(audio)\n",
        "    except:\n",
        "      df.loc[i,'fake_transcription'] = ''\n",
        "\n",
        "  # Calculate word error rate  \n",
        "  df.loc[i, 'wer'] = wer(sentence, df.loc[i, 'fake_transcription'], debug=False)['WER']\n",
        "\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gd3A2ksRTeDW"
      },
      "outputs": [],
      "source": [
        "print(f\"The average word error rate (WER) is {df.loc[:, 'wer'].mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Speaker classification accuracy"
      ],
      "metadata": {
        "id": "fnY70qSuKj3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/data/fake'\n",
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=64000, # Set the length of the output to be 4 seconds\n",
        "    subset='both')\n",
        "\n",
        "label_names = np.array(train_ds.class_names)\n",
        "\n",
        "print(\"\\nlabel names:\", label_names)"
      ],
      "metadata": {
        "id": "RPZk6KZSKeNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def squeeze(audio, labels):\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  return audio, labels\n",
        "\n",
        "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "SjkbUxEKKiVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Break up val_ds into a validation and test set\n",
        "test_ds = val_ds.shard(num_shards=2, index=0)\n",
        "val_ds = val_ds.shard(num_shards=2, index=1)"
      ],
      "metadata": {
        "id": "XtEZNmciLwWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for example_audio, example_labels in train_ds.take(1):  \n",
        "  print(example_audio.shape)\n",
        "  print(example_labels.shape)"
      ],
      "metadata": {
        "id": "fr5HT1osLyfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert waveforms to spectograms\n",
        "def get_spectrogram(waveform):\n",
        "  # Convert the waveform to a spectrogram via a STFT.\n",
        "  spectrogram = tf.signal.stft(\n",
        "      waveform, frame_length=255, frame_step=128)\n",
        "  # Obtain the magnitude of the STFT.\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # Add a `channels` dimension, so that the spectrogram can be used\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram"
      ],
      "metadata": {
        "id": "Srd5-SPlL3Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(6):\n",
        "  label = label_names[example_labels[i]]\n",
        "  waveform = example_audio[i]\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "  print('Label:', label)\n",
        "  print('Waveform shape:', waveform.shape)\n",
        "  print('Spectrogram shape:', spectrogram.shape)\n",
        "  print('Audio playback')\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ],
      "metadata": {
        "id": "FI9jAQnRL5hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)"
      ],
      "metadata": {
        "id": "0G1jCZQOL8Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create spectrogram datasets from the audio datasets"
      ],
      "metadata": {
        "id": "OppERzncMDqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_spec_ds(ds):\n",
        "  return ds.map(\n",
        "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  \n",
        "train_spectrogram_ds = make_spec_ds(train_ds)\n",
        "val_spectrogram_ds = make_spec_ds(val_ds)\n",
        "test_spectrogram_ds = make_spec_ds(test_ds)\n",
        "\n",
        "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
        "  break\n",
        "\n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
        "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jHusWXx_MF01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets train the model"
      ],
      "metadata": {
        "id": "F1o1RSejMY-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "input_shape = example_spectrograms.shape[1:]\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(label_names)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalize.\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "3AzKTHngMaLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "tMuvrT0iM0ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ],
      "metadata": {
        "id": "U3_RixFJM1uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "metrics = history.history\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ucufHwbhM28g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_spectrogram_ds, return_dict=True, verbose=True)\n",
        "y_pred = model.predict(test_spectrogram_ds)\n",
        "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)\n",
        "y_labels = np.concatenate([y for x, y in test_spectrogram_ds], axis=0)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(test_spectrogram_ds)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_labels, y_pred_bool))"
      ],
      "metadata": {
        "id": "XEAJFAKCrxz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRtQMb8hjE-P"
      },
      "source": [
        "#### Strategy 2: Voice cloning with YourTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfrj3-BKcime"
      },
      "outputs": [],
      "source": [
        "# List available 🐸TTS models and choose the first one\n",
        "model_name = TTS.list_models()[0]\n",
        "\n",
        "# Init TTS\n",
        "tts = TTS(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPoZG4pBZKsF"
      },
      "outputs": [],
      "source": [
        "train_paths = list(paths.list_files(\"/content/timit/archive/data/TRAIN/\", \".wav\", \".WAV.wav\"))\n",
        "i=0\n",
        "# For each file in TRAIN\n",
        "for path in train_paths:\n",
        "  tts.tts_to_file(df.sample()['sentence'].values[0], speaker_wav=path, language=\"en\", file_path=f\"data/fake/{i}.wav\")\n",
        "\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMC63OW8VO8g"
      },
      "outputs": [],
      "source": [
        "print(f'There are {len(list(paths.list_files(\"/content/data/fake\", \".wav\")))} generated fake samples.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU1Q5_Ohlf1p"
      },
      "outputs": [],
      "source": [
        "Audio('output.wav', autoplay=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUqWZSZPeJGN"
      },
      "source": [
        "#### Strategy 3: Use TIMIT Dataset to clone voices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQlRfOPQyPJe"
      },
      "outputs": [],
      "source": [
        "# Implement the WaveNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtOR1NGpiCwq"
      },
      "outputs": [],
      "source": [
        "def generate_fake(source_speaker, target_speaker, text) ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8lvLR3SiXci"
      },
      "outputs": [],
      "source": [
        "# Collect 1600 positive samples (from the CommonVoice dataset)\n",
        "\n",
        "\n",
        "# Generate negative samples (from voice cloning system)\n",
        "\n",
        "\n",
        "# Train a model to identify real and fake audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i45-xQLaiFZK"
      },
      "source": [
        "### Build a fake audio detection system\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZc0wMffRRp-"
      },
      "source": [
        "We can load the data using `keras.utils.audio_dataset_from_directory`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgghw120QCvk"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/data'\n",
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=64000, # Set the length of the output to be 4 seconds\n",
        "    subset='both')\n",
        "\n",
        "label_names = np.array(train_ds.class_names)\n",
        "\n",
        "print(\"\\nlabel names:\", label_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0BsF_ZDWDNo"
      },
      "outputs": [],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAAFFbQm4rWP"
      },
      "source": [
        "This dataset only contains single channel audio, so use the `tf.squeeze` function to drop the extra axis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-prpXi74nr3"
      },
      "outputs": [],
      "source": [
        "def squeeze(audio, labels):\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  return audio, labels\n",
        "\n",
        "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7H0xnIN4sRV"
      },
      "outputs": [],
      "source": [
        "# Break up val_ds into a validation and test set\n",
        "test_ds = val_ds.shard(num_shards=2, index=0)\n",
        "val_ds = val_ds.shard(num_shards=2, index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFSY4sUZ6lkR"
      },
      "outputs": [],
      "source": [
        "for example_audio, example_labels in train_ds.take(1):  \n",
        "  print(example_audio.shape)\n",
        "  print(example_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiQ20xrM46Ui"
      },
      "outputs": [],
      "source": [
        "# Convert waveforms to spectograms\n",
        "def get_spectrogram(waveform):\n",
        "  # Convert the waveform to a spectrogram via a STFT.\n",
        "  spectrogram = tf.signal.stft(\n",
        "      waveform, frame_length=255, frame_step=128)\n",
        "  # Obtain the magnitude of the STFT.\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # Add a `channels` dimension, so that the spectrogram can be used\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAxFCNUFXqJu"
      },
      "outputs": [],
      "source": [
        "label_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weKtXWiK4_Og"
      },
      "outputs": [],
      "source": [
        "for i in range(6):\n",
        "  label = label_names[example_labels[i]]\n",
        "  waveform = example_audio[i]\n",
        "  spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "  print('Label:', label)\n",
        "  print('Waveform shape:', waveform.shape)\n",
        "  print('Spectrogram shape:', spectrogram.shape)\n",
        "  print('Audio playback')\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1MkAZ1o8Og0"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "  if len(spectrogram.shape) > 2:\n",
        "    assert len(spectrogram.shape) == 3\n",
        "    spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "  # Convert the frequencies to log scale and transpose, so that the time is\n",
        "  # represented on the x-axis (columns).\n",
        "  # Add an epsilon to avoid taking a log of zero.\n",
        "  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "  height = log_spec.shape[0]\n",
        "  width = log_spec.shape[1]\n",
        "  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "  Y = range(height)\n",
        "  ax.pcolormesh(X, Y, log_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY2wlytN7M-I"
      },
      "source": [
        "Now create spectrogram datasets from the audio datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQZmlWRN7QA5"
      },
      "outputs": [],
      "source": [
        "def make_spec_ds(ds):\n",
        "  return ds.map(\n",
        "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8X4EMrH7VBO"
      },
      "outputs": [],
      "source": [
        "train_spectrogram_ds = make_spec_ds(train_ds)\n",
        "val_spectrogram_ds = make_spec_ds(val_ds)\n",
        "test_spectrogram_ds = make_spec_ds(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MLMpf9L7pt4"
      },
      "outputs": [],
      "source": [
        "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrhR2OzH7rGH"
      },
      "outputs": [],
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
        "    ax.set_title(label_names[example_spect_labels[i].numpy()])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl1BZisA7Gxw"
      },
      "source": [
        "Now lets build and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ-IXF9_7GHn"
      },
      "outputs": [],
      "source": [
        "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "test_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjTWXcff7bJu"
      },
      "outputs": [],
      "source": [
        "input_shape = example_spectrograms.shape[1:]\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(label_names)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalize.\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aaX-0RX8g4k"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfmqjUjg8ht2"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWmjiwJ6P_A4"
      },
      "outputs": [],
      "source": [
        "metrics = history.history\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R48jm9KD8nJW"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_spectrogram_ds, return_dict=True, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUzPVqrH8qXr"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_spectrogram_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21zIJdQH8tWO"
      },
      "outputs": [],
      "source": [
        "y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfLnKMzHxz2O"
      },
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ0UItFUSY7b"
      },
      "outputs": [],
      "source": [
        "y_labels = np.concatenate([y for x, y in test_spectrogram_ds], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IInWiwzRSfpK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(test_spectrogram_ds)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_labels, y_pred_bool))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFCeBTXKXRHZ"
      },
      "outputs": [],
      "source": [
        "print(y_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o8jPvAkgkCj"
      },
      "outputs": [],
      "source": [
        "np.unique(y_labels, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTPJv3vasyWM"
      },
      "outputs": [],
      "source": [
        "y_pred_val = model.predict(test_spectrogram_ds)\n",
        "y_pred_val_bool = np.argmax(y_pred_val, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajHvREdotCq8"
      },
      "outputs": [],
      "source": [
        "np.argmax(model.predict(val_spectrogram_ds), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twH28JVftCCk"
      },
      "outputs": [],
      "source": [
        "np.argmax(model.predict(test_spectrogram_ds), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AGKq22Nrhgf"
      },
      "source": [
        "### Run inference with my voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgt9MU9DPGGE"
      },
      "outputs": [],
      "source": [
        "list(paths.list_files(\"/content/data/me\", \".wav\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mc5n9KfYwS5"
      },
      "outputs": [],
      "source": [
        "# TODO: Record an audio (not fake) and do an inference with the model\n",
        "my_paths = list(paths.list_files(\"/content/data/me\", \".wav\"))\n",
        "\n",
        "for path in my_paths:\n",
        "  x = tf.io.read_file(str(path))\n",
        "  x, sample_rate = tf.audio.decode_wav(x, desired_samples=64000,)\n",
        "  x = tf.squeeze(x, axis=-1)\n",
        "  waveform = x\n",
        "  x = get_spectrogram(x)\n",
        "  x = x[tf.newaxis,...]\n",
        "\n",
        "  %matplotlib inline\n",
        "  prediction = model(x)\n",
        "  x_labels = ['fake', 'real']\n",
        "  plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
        "  plt.title('Voice Clip Prediction')\n",
        "  plt.show()\n",
        "\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp16uykmQlEF"
      },
      "source": [
        "Now lets use the voice cloner to generate fake samples of my voice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4hYV9LUPeZd"
      },
      "outputs": [],
      "source": [
        "real_voice_paths = list(paths.list_files(\"/content/data/me\", \".wav\"))\n",
        "\n",
        "i=0\n",
        "for path in real_voice_paths:\n",
        "  tts.tts_to_file(df.sample()['sentence'].values[0], speaker_wav=path, language=\"en\", file_path=f\"data/me_fake/{i}.wav\")\n",
        "  i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gfGcsogTCpg"
      },
      "outputs": [],
      "source": [
        "my_paths = list(paths.list_files(\"/content/data/me_fake\", \".wav\"))\n",
        "\n",
        "for path in my_paths:\n",
        "  x = tf.io.read_file(str(path))\n",
        "  x, sample_rate = tf.audio.decode_wav(x, desired_samples=64000,)\n",
        "  x = tf.squeeze(x, axis=-1)\n",
        "  waveform = x\n",
        "  x = get_spectrogram(x)\n",
        "  x = x[tf.newaxis,...]\n",
        "\n",
        "  %matplotlib inline\n",
        "  prediction = model(x)\n",
        "  x_labels = ['fake', 'real']\n",
        "  plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
        "  plt.title('Voice Clip Prediction')\n",
        "  plt.show()\n",
        "\n",
        "  display.display(display.Audio(waveform, rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l56zHAIph5u"
      },
      "source": [
        "### Summary\n",
        "(TODO)\n",
        "Further considerations:\n",
        "- Real world samples contain background noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVU-J4hdnz_g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PUqWZSZPeJGN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}